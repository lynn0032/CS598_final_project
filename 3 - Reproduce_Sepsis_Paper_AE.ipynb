{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3 - Reproduce_Sepsis_Paper_AE.ipynb","provenance":[],"mount_file_id":"1k22VhthrW-upWzLySHgWE53otqazaH5e","authorship_tag":"ABX9TyMuyOrzA/u+ZDQA55SscdLA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Here, the data is loaded into a pandas dataframe from my Google drive. The data is posted with the paper by Hou et al., at https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02620-5#availability-of-data-and-materials"],"metadata":{"id":"o9eq3DTyUFoB"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"TAXw_zT2MzzG","executionInfo":{"status":"ok","timestamp":1652045482056,"user_tz":300,"elapsed":1361,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"outputs":[],"source":["import pandas as pd\n","\n","sepsis_df = pd.read_csv(\"/content/drive/MyDrive/CS 598 - Deep Learning for Healthcare/Final Project/Reproducing Septic Paper/sepsis_data.csv\")"]},{"cell_type":"markdown","source":["Here, we can see the columns from this dataset, which include demographic information, vital signs, and lab results."],"metadata":{"id":"Qkl9UJplUTkx"}},{"cell_type":"code","source":["for col in sepsis_df.columns:\n","  print(col)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j84FJBBlNItS","executionInfo":{"status":"ok","timestamp":1652045483342,"user_tz":300,"elapsed":193,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"6328abb6-36b2-4d27-ba85-b6046913e052"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["icustay_id\n","hadm_id\n","intime\n","outtime\n","dbsource\n","suspected_infection_time_poe\n","suspected_infection_time_poe_days\n","specimen_poe\n","positiveculture_poe\n","antibiotic_time_poe\n","blood_culture_time\n","blood_culture_positive\n","age\n","gender\n","is_male\n","ethnicity\n","race_white\n","race_black\n","race_hispanic\n","race_other\n","metastatic_cancer\n","diabetes\n","first_service\n","hospital_expire_flag\n","thirtyday_expire_flag\n","icu_los\n","hosp_los\n","sepsis_angus\n","sepsis_martin\n","sepsis_explicit\n","septic_shock_explicit\n","severe_sepsis_explicit\n","sepsis_nqf\n","sepsis_cdc\n","sepsis_cdc_simple\n","elixhauser_hospital\n","vent\n","sofa\n","lods\n","sirs\n","qsofa\n","qsofa_sysbp_score\n","qsofa_gcs_score\n","qsofa_resprate_score\n","aniongap_min\n","aniongap_max\n","bicarbonate_min\n","bicarbonate_max\n","creatinine_min\n","creatinine_max\n","chloride_min\n","chloride_max\n","glucose_min\n","glucose_max\n","hematocrit_min\n","hematocrit_max\n","hemoglobin_min\n","hemoglobin_max\n","lactate_min\n","lactate_max\n","lactate_mean\n","platelet_min\n","platelet_max\n","potassium_min\n","potassium_max\n","inr_min\n","inr_max\n","sodium_min\n","sodium_max\n","bun_min\n","bun_max\n","bun_mean\n","wbc_min\n","wbc_max\n","wbc_mean\n","heartrate_min\n","heartrate_max\n","heartrate_mean\n","sysbp_min\n","sysbp_max\n","sysbp_mean\n","diasbp_min\n","diasbp_max\n","diasbp_mean\n","meanbp_min\n","meanbp_max\n","meanbp_mean\n","resprate_min\n","resprate_max\n","resprate_mean\n","tempc_min\n","tempc_max\n","tempc_mean\n","spo2_min\n","spo2_max\n","spo2_mean\n","glucose_min1\n","glucose_max1\n","glucose_mean\n","rrt\n","subject_id\n","hadm_id.1\n","icustay_id.1\n","urineoutput\n","colloid_bolus\n","crystalloid_bolus\n"]}]},{"cell_type":"markdown","source":["We confirm that there are no missing values in the target, which is thirtyday_expire_flag, indicating whether or not the patient died within 30 days."],"metadata":{"id":"XB-ZIKDAHNf8"}},{"cell_type":"code","source":["sepsis_df[\"thirtyday_expire_flag\"].isna().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLeQ6wkeHFXq","executionInfo":{"status":"ok","timestamp":1652045484384,"user_tz":300,"elapsed":198,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"d8e18dcf-e169-4dbd-fc55-181c6bade377"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["We look at the value counts for the target, seeing that there are 3670 patients who survived, and 889 patients who died within 30 days. We see that we have a class imbalance, with 80.5% of patients surviving."],"metadata":{"id":"QOlTisjTUmus"}},{"cell_type":"code","source":["sepsis_df[\"thirtyday_expire_flag\"].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZ_CN0PENLII","executionInfo":{"status":"ok","timestamp":1652045485109,"user_tz":300,"elapsed":3,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"752b960d-2a55-43df-ac46-2676f333f4b7"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    3670\n","1     889\n","Name: thirtyday_expire_flag, dtype: int64"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Now, we select the target."],"metadata":{"id":"poMpN8lRPnwQ"}},{"cell_type":"code","source":["y = sepsis_df[\"thirtyday_expire_flag\"]"],"metadata":{"id":"NdYn1Ze5PnMm","executionInfo":{"status":"ok","timestamp":1652045485914,"user_tz":300,"elapsed":1,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Next, we drop columns that are not used for prediction. Note that gender and ethnicity are one-hot encoded, and these one-hot encoded variables are still included in the dataset."],"metadata":{"id":"UwdsTnwsO2GJ"}},{"cell_type":"code","source":["to_drop = [\"icustay_id\",\n","           \"hadm_id\",\n","           \"intime\",\n","           \"outtime\",\n","           \"dbsource\",\n","           \"suspected_infection_time_poe\",\n","           \"suspected_infection_time_poe_days\",\n","           \"specimen_poe\",\n","           \"positiveculture_poe\",\n","           \"antibiotic_time_poe\",\n","           \"blood_culture_time\",\n","           \"blood_culture_positive\",\n","           \"gender\",\n","           \"ethnicity\",\n","           \"hospital_expire_flag\",\n","           \"thirtyday_expire_flag\",\n","           \"first_service\"]\n","\n","X = sepsis_df.drop(columns = to_drop)"],"metadata":{"id":"YhxCrv2tOEff","executionInfo":{"status":"ok","timestamp":1652045486983,"user_tz":300,"elapsed":203,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["We are now left with only numerical features. We perform mean value imputation on all columns to fill in missing values."],"metadata":{"id":"sqSHETGrHQMl"}},{"cell_type":"code","source":["X = X.apply(lambda x:x.fillna(x.mean()), axis = 0).values"],"metadata":{"id":"L7sXicHEHRbg","executionInfo":{"status":"ok","timestamp":1652045487758,"user_tz":300,"elapsed":2,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Now, we ust the StandardScaler from SciKit-Learn to normalize the data so that each feature has mean 0 and standard deviation 1."],"metadata":{"id":"6KOCeirSVlnb"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)"],"metadata":{"id":"N17KZd1lW7Q4","executionInfo":{"status":"ok","timestamp":1652045488967,"user_tz":300,"elapsed":244,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["We will use a memory profile to check the computational requirements."],"metadata":{"id":"RNdbsxPubx5U"}},{"cell_type":"code","source":["!pip install memory_profiler\n","%load_ext memory_profiler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QoYYeJjbtcw","executionInfo":{"status":"ok","timestamp":1652045494352,"user_tz":300,"elapsed":4738,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"028752c5-92fa-4ac6-98f2-2ca8db225aca"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting memory_profiler\n","  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n","Building wheels for collected packages: memory-profiler\n","  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=5ac90980dbf2409b69a3906c93551b69ca54ebd75e8927eebe8ffa763c018e6c\n","  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n","Successfully built memory-profiler\n","Installing collected packages: memory-profiler\n","Successfully installed memory-profiler-0.60.0\n"]}]},{"cell_type":"markdown","source":["Next, an autoencoder is used for dimensionality reduction and feature construction, reducing the number of dimensions to 40. We begin by convert the data to tensors, and making a data loader."],"metadata":{"id":"OScIY1k9gNEQ"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","batch_size = 64\n","\n","X = torch.Tensor(X)\n","y = torch.LongTensor(y.values)\n","\n","dataset = TensorDataset(X, y)\n","dataloader = DataLoader(dataset, batch_size = batch_size)"],"metadata":{"id":"MFdWnXIcjzxh","executionInfo":{"status":"ok","timestamp":1652045501053,"user_tz":300,"elapsed":4189,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Here, we define the autoencoder."],"metadata":{"id":"Qfuf0mrdkkma"}},{"cell_type":"code","source":["# Define model\n","class AE(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.encoder = nn.Linear(89, 40)\n","        self.decoder = nn.Linear(40, 89)\n","\n","    def forward(self, x):\n","        #print(x.shape)\n","        x = F.relu(self.encoder(x))\n","        x = F.sigmoid(self.decoder(x))\n","        return x\n","\n","\n","model = AE()"],"metadata":{"id":"NI_kQgp-j3Qi","executionInfo":{"status":"ok","timestamp":1652045503791,"user_tz":300,"elapsed":196,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["To train the autoencoder, we use MSELoss and an Adam optimizer with learning rate 0.001."],"metadata":{"id":"HDzES3Yfkm-d"}},{"cell_type":"code","source":["loss_fn = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"u4w6y8X0j7Ht","executionInfo":{"status":"ok","timestamp":1652045506597,"user_tz":300,"elapsed":2,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["We define our training function. Note that we ignore `y` for training the autoencoder, as we are trying to compress the features."],"metadata":{"id":"O-ZfIq8jku1M"}},{"cell_type":"code","source":["def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    model.train()\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Compute reconstruction error\n","        pred = model(X)\n","        loss = loss_fn(pred, X)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"],"metadata":{"id":"p_fKd5h9j8sx","executionInfo":{"status":"ok","timestamp":1652045508488,"user_tz":300,"elapsed":398,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Now, we train the autoencoder."],"metadata":{"id":"o2KU_NQ1lGyd"}},{"cell_type":"code","source":["%%time\n","%%memit\n","\n","epochs = 100\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train(dataloader, model, loss_fn, optimizer)\n","print(\"Done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dSxxtdNokAz4","executionInfo":{"status":"ok","timestamp":1652045573296,"user_tz":300,"elapsed":11210,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"3dd8dbe1-f8dd-415b-e5a3-4c1c6ce6fbd1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","loss: 0.843112  [    0/ 4559]\n","Epoch 2\n","-------------------------------\n","loss: 0.842898  [    0/ 4559]\n","Epoch 3\n","-------------------------------\n","loss: 0.842697  [    0/ 4559]\n","Epoch 4\n","-------------------------------\n","loss: 0.842529  [    0/ 4559]\n","Epoch 5\n","-------------------------------\n","loss: 0.842338  [    0/ 4559]\n","Epoch 6\n","-------------------------------\n","loss: 0.842187  [    0/ 4559]\n","Epoch 7\n","-------------------------------\n","loss: 0.842009  [    0/ 4559]\n","Epoch 8\n","-------------------------------\n","loss: 0.841850  [    0/ 4559]\n","Epoch 9\n","-------------------------------\n","loss: 0.841690  [    0/ 4559]\n","Epoch 10\n","-------------------------------\n","loss: 0.841551  [    0/ 4559]\n","Epoch 11\n","-------------------------------\n","loss: 0.841408  [    0/ 4559]\n","Epoch 12\n","-------------------------------\n","loss: 0.841274  [    0/ 4559]\n","Epoch 13\n","-------------------------------\n","loss: 0.841135  [    0/ 4559]\n","Epoch 14\n","-------------------------------\n","loss: 0.841013  [    0/ 4559]\n","Epoch 15\n","-------------------------------\n","loss: 0.840961  [    0/ 4559]\n","Epoch 16\n","-------------------------------\n","loss: 0.841212  [    0/ 4559]\n","Epoch 17\n","-------------------------------\n","loss: 0.841151  [    0/ 4559]\n","Epoch 18\n","-------------------------------\n","loss: 0.841062  [    0/ 4559]\n","Epoch 19\n","-------------------------------\n","loss: 0.840987  [    0/ 4559]\n","Epoch 20\n","-------------------------------\n","loss: 0.840975  [    0/ 4559]\n","Epoch 21\n","-------------------------------\n","loss: 0.841004  [    0/ 4559]\n","Epoch 22\n","-------------------------------\n","loss: 0.841072  [    0/ 4559]\n","Epoch 23\n","-------------------------------\n","loss: 0.841164  [    0/ 4559]\n","Epoch 24\n","-------------------------------\n","loss: 0.841190  [    0/ 4559]\n","Epoch 25\n","-------------------------------\n","loss: 0.841245  [    0/ 4559]\n","Epoch 26\n","-------------------------------\n","loss: 0.841297  [    0/ 4559]\n","Epoch 27\n","-------------------------------\n","loss: 0.841381  [    0/ 4559]\n","Epoch 28\n","-------------------------------\n","loss: 0.841472  [    0/ 4559]\n","Epoch 29\n","-------------------------------\n","loss: 0.841527  [    0/ 4559]\n","Epoch 30\n","-------------------------------\n","loss: 0.841570  [    0/ 4559]\n","Epoch 31\n","-------------------------------\n","loss: 0.841589  [    0/ 4559]\n","Epoch 32\n","-------------------------------\n","loss: 0.841604  [    0/ 4559]\n","Epoch 33\n","-------------------------------\n","loss: 0.841609  [    0/ 4559]\n","Epoch 34\n","-------------------------------\n","loss: 0.841644  [    0/ 4559]\n","Epoch 35\n","-------------------------------\n","loss: 0.841684  [    0/ 4559]\n","Epoch 36\n","-------------------------------\n","loss: 0.841767  [    0/ 4559]\n","Epoch 37\n","-------------------------------\n","loss: 0.841882  [    0/ 4559]\n","Epoch 38\n","-------------------------------\n","loss: 0.841991  [    0/ 4559]\n","Epoch 39\n","-------------------------------\n","loss: 0.842051  [    0/ 4559]\n","Epoch 40\n","-------------------------------\n","loss: 0.842130  [    0/ 4559]\n","Epoch 41\n","-------------------------------\n","loss: 0.842183  [    0/ 4559]\n","Epoch 42\n","-------------------------------\n","loss: 0.842224  [    0/ 4559]\n","Epoch 43\n","-------------------------------\n","loss: 0.842261  [    0/ 4559]\n","Epoch 44\n","-------------------------------\n","loss: 0.842293  [    0/ 4559]\n","Epoch 45\n","-------------------------------\n","loss: 0.842328  [    0/ 4559]\n","Epoch 46\n","-------------------------------\n","loss: 0.842345  [    0/ 4559]\n","Epoch 47\n","-------------------------------\n","loss: 0.842392  [    0/ 4559]\n","Epoch 48\n","-------------------------------\n","loss: 0.842402  [    0/ 4559]\n","Epoch 49\n","-------------------------------\n","loss: 0.842463  [    0/ 4559]\n","Epoch 50\n","-------------------------------\n","loss: 0.842447  [    0/ 4559]\n","Epoch 51\n","-------------------------------\n","loss: 0.842466  [    0/ 4559]\n","Epoch 52\n","-------------------------------\n","loss: 0.842447  [    0/ 4559]\n","Epoch 53\n","-------------------------------\n","loss: 0.842480  [    0/ 4559]\n","Epoch 54\n","-------------------------------\n","loss: 0.842426  [    0/ 4559]\n","Epoch 55\n","-------------------------------\n","loss: 0.842452  [    0/ 4559]\n","Epoch 56\n","-------------------------------\n","loss: 0.842387  [    0/ 4559]\n","Epoch 57\n","-------------------------------\n","loss: 0.842408  [    0/ 4559]\n","Epoch 58\n","-------------------------------\n","loss: 0.842323  [    0/ 4559]\n","Epoch 59\n","-------------------------------\n","loss: 0.842329  [    0/ 4559]\n","Epoch 60\n","-------------------------------\n","loss: 0.842202  [    0/ 4559]\n","Epoch 61\n","-------------------------------\n","loss: 0.842230  [    0/ 4559]\n","Epoch 62\n","-------------------------------\n","loss: 0.842084  [    0/ 4559]\n","Epoch 63\n","-------------------------------\n","loss: 0.842123  [    0/ 4559]\n","Epoch 64\n","-------------------------------\n","loss: 0.841980  [    0/ 4559]\n","Epoch 65\n","-------------------------------\n","loss: 0.842021  [    0/ 4559]\n","Epoch 66\n","-------------------------------\n","loss: 0.841879  [    0/ 4559]\n","Epoch 67\n","-------------------------------\n","loss: 0.841943  [    0/ 4559]\n","Epoch 68\n","-------------------------------\n","loss: 0.841810  [    0/ 4559]\n","Epoch 69\n","-------------------------------\n","loss: 0.841870  [    0/ 4559]\n","Epoch 70\n","-------------------------------\n","loss: 0.841739  [    0/ 4559]\n","Epoch 71\n","-------------------------------\n","loss: 0.841809  [    0/ 4559]\n","Epoch 72\n","-------------------------------\n","loss: 0.841709  [    0/ 4559]\n","Epoch 73\n","-------------------------------\n","loss: 0.841772  [    0/ 4559]\n","Epoch 74\n","-------------------------------\n","loss: 0.841678  [    0/ 4559]\n","Epoch 75\n","-------------------------------\n","loss: 0.841714  [    0/ 4559]\n","Epoch 76\n","-------------------------------\n","loss: 0.841629  [    0/ 4559]\n","Epoch 77\n","-------------------------------\n","loss: 0.841657  [    0/ 4559]\n","Epoch 78\n","-------------------------------\n","loss: 0.841587  [    0/ 4559]\n","Epoch 79\n","-------------------------------\n","loss: 0.841609  [    0/ 4559]\n","Epoch 80\n","-------------------------------\n","loss: 0.841527  [    0/ 4559]\n","Epoch 81\n","-------------------------------\n","loss: 0.841548  [    0/ 4559]\n","Epoch 82\n","-------------------------------\n","loss: 0.841467  [    0/ 4559]\n","Epoch 83\n","-------------------------------\n","loss: 0.841470  [    0/ 4559]\n","Epoch 84\n","-------------------------------\n","loss: 0.841415  [    0/ 4559]\n","Epoch 85\n","-------------------------------\n","loss: 0.841405  [    0/ 4559]\n","Epoch 86\n","-------------------------------\n","loss: 0.841353  [    0/ 4559]\n","Epoch 87\n","-------------------------------\n","loss: 0.841333  [    0/ 4559]\n","Epoch 88\n","-------------------------------\n","loss: 0.841282  [    0/ 4559]\n","Epoch 89\n","-------------------------------\n","loss: 0.841271  [    0/ 4559]\n","Epoch 90\n","-------------------------------\n","loss: 0.841209  [    0/ 4559]\n","Epoch 91\n","-------------------------------\n","loss: 0.841204  [    0/ 4559]\n","Epoch 92\n","-------------------------------\n","loss: 0.841147  [    0/ 4559]\n","Epoch 93\n","-------------------------------\n","loss: 0.841146  [    0/ 4559]\n","Epoch 94\n","-------------------------------\n","loss: 0.841097  [    0/ 4559]\n","Epoch 95\n","-------------------------------\n","loss: 0.841085  [    0/ 4559]\n","Epoch 96\n","-------------------------------\n","loss: 0.841049  [    0/ 4559]\n","Epoch 97\n","-------------------------------\n","loss: 0.841039  [    0/ 4559]\n","Epoch 98\n","-------------------------------\n","loss: 0.840999  [    0/ 4559]\n","Epoch 99\n","-------------------------------\n","loss: 0.841004  [    0/ 4559]\n","Epoch 100\n","-------------------------------\n","loss: 0.840969  [    0/ 4559]\n","Done!\n","peak memory: 455.53 MiB, increment: 0.12 MiB\n","CPU times: user 9.66 s, sys: 76.8 ms, total: 9.74 s\n","Wall time: 11.2 s\n"]}]},{"cell_type":"markdown","source":["Now that we've trained the autoencoder, we use the encoder alone to transform the features."],"metadata":{"id":"Bh_-L4K-l8Tm"}},{"cell_type":"code","source":["enc_X = F.relu(model.encoder(X)).detach().numpy()"],"metadata":{"id":"fZBCZv4gkGNL","executionInfo":{"status":"ok","timestamp":1652045655843,"user_tz":300,"elapsed":192,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["We split the data into a training set and a testing set, which we will use to train and evaluate models. The training set is 80% of the data, while the testing set is 20% of the data."],"metadata":{"id":"rzUEK5TWVsmR"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(enc_X, y, test_size = .2, stratify = y, random_state = 0)"],"metadata":{"id":"Fc-Qt_G3PXtM","executionInfo":{"status":"ok","timestamp":1652045659840,"user_tz":300,"elapsed":204,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["Next, we train our first model, which is logistic regression. We use the model classifier from SciKit-Learn, and fit it to the training data. We evaluate the performance by computing the accuracy, precision, recall, and AUC on the testing data."],"metadata":{"id":"qtZ6mOn4WEdm"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","\n","clf = LogisticRegression(random_state = 0, max_iter = 10000)\n","clf.fit(X_train, y_train)\n","\n","y_pred = clf.predict(X_test)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Precision:\", precision_score(y_test, y_pred))\n","print(\"Recall:\", recall_score(y_test, y_pred))\n","print(\"AUC:\", roc_auc_score(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LsjRvJLGUEQV","executionInfo":{"status":"ok","timestamp":1652045662383,"user_tz":300,"elapsed":412,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"69756806-93f2-453e-fee3-d8721ed15f03"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8651315789473685\n","Precision: 0.7570093457943925\n","Recall: 0.4550561797752809\n","AUC: 0.7098169182255151\n"]}]},{"cell_type":"markdown","source":["Our next model is a support vector machine classifier. Again, we use the model from SciKit-Learn. Below, I include the SVC with the default kernel: rbf. I experimented with other kernels, and found that this gave the best performance."],"metadata":{"id":"YOu8kTQsWeXA"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","clf = SVC(random_state = 0)\n","clf.fit(X_train, y_train)\n","\n","y_pred = clf.predict(X_test)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Precision:\", precision_score(y_test, y_pred))\n","print(\"Recall:\", recall_score(y_test, y_pred))\n","print(\"AUC:\", roc_auc_score(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xQIaM7xYUlz","executionInfo":{"status":"ok","timestamp":1652045669709,"user_tz":300,"elapsed":632,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"23d6f2d5-f199-4f7d-fed5-c3ad64a04dd9"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8662280701754386\n","Precision: 0.9\n","Recall: 0.3539325842696629\n","AUC: 0.6721978997642591\n"]}]},{"cell_type":"markdown","source":["Next, I used a random forest classifier."],"metadata":{"id":"YI7RyN3kWyrG"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","clf = RandomForestClassifier(random_state = 0)\n","clf.fit(X_train, y_train)\n","\n","y_pred = clf.predict(X_test)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Precision:\", precision_score(y_test, y_pred))\n","print(\"Recall:\", recall_score(y_test, y_pred))\n","print(\"AUC:\", roc_auc_score(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQxRY2rNYmLK","executionInfo":{"status":"ok","timestamp":1652045677718,"user_tz":300,"elapsed":2631,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"1e8d6163-b4f0-467e-8eb5-406f0a315c37"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8618421052631579\n","Precision: 0.9333333333333333\n","Recall: 0.3146067415730337\n","AUC: 0.6545785751461899\n"]}]},{"cell_type":"markdown","source":["Now, we try a gradient boosting classifier. Note that the result should be essentially the same as XGBoost, except that XGBoost typically runs faster."],"metadata":{"id":"pr5wdMnNW66v"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","clf = GradientBoostingClassifier(random_state = 0)\n","clf.fit(X_train, y_train)\n","\n","y_pred = clf.predict(X_test)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Precision:\", precision_score(y_test, y_pred))\n","print(\"Recall:\", recall_score(y_test, y_pred))\n","print(\"AUC:\", roc_auc_score(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtwqDxf1Y7iJ","executionInfo":{"status":"ok","timestamp":1652045683917,"user_tz":300,"elapsed":5050,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"5f97f8cc-3df4-4ea0-d643-51437524c6e3"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8607456140350878\n","Precision: 0.8311688311688312\n","Recall: 0.3595505617977528\n","AUC: 0.6709196950678138\n"]}]},{"cell_type":"markdown","source":["Next, we try a $k$-nearest neighbors classifier. From experimenting with the value our $k$, we found that $k = 1$ gave the best performance (though this is most likely overfitting)."],"metadata":{"id":"JXSSQFTAXhaH"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","clf = KNeighborsClassifier(n_neighbors = 1)\n","clf.fit(X_train, y_train)\n","\n","y_pred = clf.predict(X_test)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Precision:\", precision_score(y_test, y_pred))\n","print(\"Recall:\", recall_score(y_test, y_pred))\n","print(\"AUC:\", roc_auc_score(y_test, y_pred))"],"metadata":{"id":"w2J8hMDFbTq-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652045685422,"user_tz":300,"elapsed":187,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"63d08e51-9e06-49e3-b355-383d25ee78b1"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8081140350877193\n","Precision: 0.5114503816793893\n","Recall: 0.37640449438202245\n","AUC: 0.6446055169457796\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"1syzRzrcXxWQ"},"execution_count":null,"outputs":[]}]}