{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4 - Reproduce_Sepsis_Paper_CNN.ipynb","provenance":[],"mount_file_id":"1ZePhnGO6-nD2Km35K-eVXQVNihGbG9P0","authorship_tag":"ABX9TyN8Zg9HhUf6uG6vNkZyXJtM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Here, the data is loaded into a pandas dataframe from my Google drive. The data is posted with the paper by Hou et al., at https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02620-5#availability-of-data-and-materials"],"metadata":{"id":"o9eq3DTyUFoB"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"TAXw_zT2MzzG","executionInfo":{"status":"ok","timestamp":1652046313065,"user_tz":300,"elapsed":1299,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"outputs":[],"source":["import pandas as pd\n","\n","sepsis_df = pd.read_csv(\"/content/drive/MyDrive/CS 598 - Deep Learning for Healthcare/Final Project/Reproducing Septic Paper/sepsis_data.csv\")"]},{"cell_type":"markdown","source":["Here, we can see the columns from this dataset, which include demographic information, vital signs, and lab results."],"metadata":{"id":"Qkl9UJplUTkx"}},{"cell_type":"code","source":["for col in sepsis_df.columns:\n","  print(col)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j84FJBBlNItS","executionInfo":{"status":"ok","timestamp":1652046315634,"user_tz":300,"elapsed":197,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"3e508e0a-9427-4cc9-8c38-e7248818fc0e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["icustay_id\n","hadm_id\n","intime\n","outtime\n","dbsource\n","suspected_infection_time_poe\n","suspected_infection_time_poe_days\n","specimen_poe\n","positiveculture_poe\n","antibiotic_time_poe\n","blood_culture_time\n","blood_culture_positive\n","age\n","gender\n","is_male\n","ethnicity\n","race_white\n","race_black\n","race_hispanic\n","race_other\n","metastatic_cancer\n","diabetes\n","first_service\n","hospital_expire_flag\n","thirtyday_expire_flag\n","icu_los\n","hosp_los\n","sepsis_angus\n","sepsis_martin\n","sepsis_explicit\n","septic_shock_explicit\n","severe_sepsis_explicit\n","sepsis_nqf\n","sepsis_cdc\n","sepsis_cdc_simple\n","elixhauser_hospital\n","vent\n","sofa\n","lods\n","sirs\n","qsofa\n","qsofa_sysbp_score\n","qsofa_gcs_score\n","qsofa_resprate_score\n","aniongap_min\n","aniongap_max\n","bicarbonate_min\n","bicarbonate_max\n","creatinine_min\n","creatinine_max\n","chloride_min\n","chloride_max\n","glucose_min\n","glucose_max\n","hematocrit_min\n","hematocrit_max\n","hemoglobin_min\n","hemoglobin_max\n","lactate_min\n","lactate_max\n","lactate_mean\n","platelet_min\n","platelet_max\n","potassium_min\n","potassium_max\n","inr_min\n","inr_max\n","sodium_min\n","sodium_max\n","bun_min\n","bun_max\n","bun_mean\n","wbc_min\n","wbc_max\n","wbc_mean\n","heartrate_min\n","heartrate_max\n","heartrate_mean\n","sysbp_min\n","sysbp_max\n","sysbp_mean\n","diasbp_min\n","diasbp_max\n","diasbp_mean\n","meanbp_min\n","meanbp_max\n","meanbp_mean\n","resprate_min\n","resprate_max\n","resprate_mean\n","tempc_min\n","tempc_max\n","tempc_mean\n","spo2_min\n","spo2_max\n","spo2_mean\n","glucose_min1\n","glucose_max1\n","glucose_mean\n","rrt\n","subject_id\n","hadm_id.1\n","icustay_id.1\n","urineoutput\n","colloid_bolus\n","crystalloid_bolus\n"]}]},{"cell_type":"markdown","source":["We confirm that there are no missing values in the target, which is thirtyday_expire_flag, indicating whether or not the patient died within 30 days."],"metadata":{"id":"XB-ZIKDAHNf8"}},{"cell_type":"code","source":["sepsis_df[\"thirtyday_expire_flag\"].isna().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLeQ6wkeHFXq","executionInfo":{"status":"ok","timestamp":1652046317483,"user_tz":300,"elapsed":212,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"a0e72529-d221-44f2-8a21-6209284acd75"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["We look at the value counts for the target, seeing that there are 3670 patients who survived, and 889 patients who died within 30 days. We see that we have a class imbalance, with 80.5% of patients surviving."],"metadata":{"id":"QOlTisjTUmus"}},{"cell_type":"code","source":["sepsis_df[\"thirtyday_expire_flag\"].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZ_CN0PENLII","executionInfo":{"status":"ok","timestamp":1652046318306,"user_tz":300,"elapsed":3,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"0e26e020-2a5b-472d-ae39-d116ddbfab1e"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    3670\n","1     889\n","Name: thirtyday_expire_flag, dtype: int64"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Now, we select the target."],"metadata":{"id":"poMpN8lRPnwQ"}},{"cell_type":"code","source":["y = sepsis_df[\"thirtyday_expire_flag\"]"],"metadata":{"id":"NdYn1Ze5PnMm","executionInfo":{"status":"ok","timestamp":1652046319503,"user_tz":300,"elapsed":2,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Next, we drop columns that are not used for prediction. Note that gender and ethnicity are one-hot encoded, and these one-hot encoded variables are still included in the dataset."],"metadata":{"id":"UwdsTnwsO2GJ"}},{"cell_type":"code","source":["to_drop = [\"icustay_id\",\n","           \"hadm_id\",\n","           \"intime\",\n","           \"outtime\",\n","           \"dbsource\",\n","           \"suspected_infection_time_poe\",\n","           \"suspected_infection_time_poe_days\",\n","           \"specimen_poe\",\n","           \"positiveculture_poe\",\n","           \"antibiotic_time_poe\",\n","           \"blood_culture_time\",\n","           \"blood_culture_positive\",\n","           \"gender\",\n","           \"ethnicity\",\n","           \"hospital_expire_flag\",\n","           \"thirtyday_expire_flag\",\n","           \"first_service\"]\n","\n","X = sepsis_df.drop(columns = to_drop)"],"metadata":{"id":"YhxCrv2tOEff","executionInfo":{"status":"ok","timestamp":1652046320460,"user_tz":300,"elapsed":2,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["We are now left with only numerical features. We perform mean value imputation on all columns to fill in missing values."],"metadata":{"id":"sqSHETGrHQMl"}},{"cell_type":"code","source":["X = X.apply(lambda x:x.fillna(x.mean()), axis = 0).values"],"metadata":{"id":"L7sXicHEHRbg","executionInfo":{"status":"ok","timestamp":1652046322443,"user_tz":300,"elapsed":192,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Now, we ust the StandardScaler from SciKit-Learn to normalize the data so that each feature has mean 0 and standard deviation 1."],"metadata":{"id":"6KOCeirSVlnb"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)"],"metadata":{"id":"N17KZd1lW7Q4","executionInfo":{"status":"ok","timestamp":1652046323813,"user_tz":300,"elapsed":439,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["We split the data into a training set and a testing set, which we will use to train and evaluate models. The training set is 80% of the data, while the testing set is 20% of the data."],"metadata":{"id":"yNFQUNNFoSFy"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, stratify = y, random_state = 0)"],"metadata":{"id":"CwMyvXhgoRTq","executionInfo":{"status":"ok","timestamp":1652046329090,"user_tz":300,"elapsed":207,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["We will use a memory profile to check the computational requirements."],"metadata":{"id":"RNdbsxPubx5U"}},{"cell_type":"code","source":["!pip install memory_profiler\n","%load_ext memory_profiler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QoYYeJjbtcw","executionInfo":{"status":"ok","timestamp":1652046340200,"user_tz":300,"elapsed":9406,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"40b53d9c-4e17-4da7-e40a-0708d2471334"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting memory_profiler\n","  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n","Building wheels for collected packages: memory-profiler\n","  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=b2af9f2d3bec475fa4828a9fd4d5a80aa6c447fa38c9888bc18dc8958dbb6aa3\n","  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n","Successfully built memory-profiler\n","Installing collected packages: memory-profiler\n","Successfully installed memory-profiler-0.60.0\n"]}]},{"cell_type":"markdown","source":["Next, we use a CNN. We first convert the data to tensors, and make the data loaders."],"metadata":{"id":"OScIY1k9gNEQ"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","batch_size = 64\n","\n","X_train = torch.Tensor(X_train)\n","y_train = torch.LongTensor(y_train.values)\n","\n","X_train = torch.unsqueeze(X_train, 1)\n","\n","train_dataset = TensorDataset(X_train, y_train)\n","train_dataloader = DataLoader(train_dataset, batch_size = batch_size)\n","\n","X_test = torch.Tensor(X_test)\n","y_test = torch.LongTensor(y_test.values)\n","\n","X_test = torch.unsqueeze(X_test, 1)\n","\n","test_dataset = TensorDataset(X_test, y_test)\n","test_dataloader = DataLoader(test_dataset, batch_size = batch_size)"],"metadata":{"id":"MFdWnXIcjzxh","executionInfo":{"status":"ok","timestamp":1652046346905,"user_tz":300,"elapsed":4223,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Here, we define the CNN. Modifications from the paper by Perng et al. are using kernels sizes of 3, and changing the number of neurons in the first fully connected layer to 1024."],"metadata":{"id":"Qfuf0mrdkkma"}},{"cell_type":"code","source":["# Define model\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv1d(1, 8, 3, padding = 1)   # in channels, out channels, kernel size\n","        self.conv2 = nn.Conv1d(8, 64, 3, padding = 1)\n","        self.conv3 = nn.Conv1d(64, 512, 3, padding = 1)\n","\n","        self.pool = nn.MaxPool1d(2, 2)    # kernel size, stride\n","\n","        self.fc1 = nn.Linear(5632, 1024)\n","        self.fc2 = nn.Linear(1024, 128)\n","        self.fc3 = nn.Linear(128, 2)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.softmax(self.fc3(x))\n","        return x\n","\n","\n","model = CNN()"],"metadata":{"id":"NI_kQgp-j3Qi","executionInfo":{"status":"ok","timestamp":1652047315883,"user_tz":300,"elapsed":195,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["To train the autoencoder, we use MSELoss and an Adam optimizer with learning rate 0.001."],"metadata":{"id":"HDzES3Yfkm-d"}},{"cell_type":"code","source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"u4w6y8X0j7Ht","executionInfo":{"status":"ok","timestamp":1652047316909,"user_tz":300,"elapsed":193,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["We define our training function. Note that we ignore `y` for training the autoencoder, as we are trying to compress the features."],"metadata":{"id":"O-ZfIq8jku1M"}},{"cell_type":"code","source":["def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    model.train()\n","    for batch, (X, y) in enumerate(dataloader):\n","\n","        # Compute prediction error\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"],"metadata":{"id":"p_fKd5h9j8sx","executionInfo":{"status":"ok","timestamp":1652047317843,"user_tz":300,"elapsed":3,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def test(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss, correct = 0, 0\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"],"metadata":{"id":"WpvAStRYnxfH","executionInfo":{"status":"ok","timestamp":1652047318063,"user_tz":300,"elapsed":2,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["Now, we train the autoencoder."],"metadata":{"id":"o2KU_NQ1lGyd"}},{"cell_type":"code","source":["%%time\n","%%memit\n","\n","epochs = 10\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train(train_dataloader, model, loss_fn, optimizer)\n","    test(test_dataloader, model, loss_fn)\n","print(\"Done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dSxxtdNokAz4","executionInfo":{"status":"ok","timestamp":1652047423062,"user_tz":300,"elapsed":103604,"user":{"displayName":"Melissa Lynn","userId":"10784518892206740494"}},"outputId":"17a907ee-ddf6-4c3a-bfe8-d0c553462acc"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"output_type":"stream","name":"stdout","text":["loss: 0.686249  [    0/ 3647]\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.508053 \n","\n","Epoch 2\n","-------------------------------\n","loss: 0.563262  [    0/ 3647]\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.508053 \n","\n","Epoch 3\n","-------------------------------\n","loss: 0.563262  [    0/ 3647]\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.508053 \n","\n","Epoch 4\n","-------------------------------\n","loss: 0.563262  [    0/ 3647]\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.508053 \n","\n","Epoch 5\n","-------------------------------\n","loss: 0.563262  [    0/ 3647]\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.508053 \n","\n","Epoch 6\n","-------------------------------\n","loss: 0.563262  [    0/ 3647]\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.508053 \n","\n","Epoch 7\n","-------------------------------\n","loss: 0.563262  [    0/ 3647]\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.508053 \n","\n","Epoch 8\n","-------------------------------\n","loss: 0.563262  [    0/ 3647]\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.508053 \n","\n","Epoch 9\n","-------------------------------\n","loss: 0.563262  [    0/ 3647]\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.508053 \n","\n","Epoch 10\n","-------------------------------\n","loss: 0.563262  [    0/ 3647]\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.508053 \n","\n","Done!\n","peak memory: 693.91 MiB, increment: 89.08 MiB\n","CPU times: user 1min 42s, sys: 837 ms, total: 1min 43s\n","Wall time: 1min 43s\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"crBquMKQsY6W"},"execution_count":null,"outputs":[]}]}